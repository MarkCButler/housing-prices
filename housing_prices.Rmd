---
title: 'Data analytics for the Ames Housing Dataset'
output:
  html_document:
    code_folding: hide
    css: style.css
    highlight: tango
    number_sections: true
    theme: yeti
    toc: true
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

```{js, echo=FALSE}
// Add the Kaggle competition banner to the top of the page.  JavaScript is
// used because the formatting options for R markdown don't appear to include an
// option to put the image above the document title.
let banner_html = '<img id="banner" src="images/housesbanner.png" alt="Banner">';
header = document.getElementById('header');
header.innerHTML = banner_html + header.innerHTML;
```

```{r, setup, message=FALSE, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    fig.align = 'center',
    comment = NA
)

library(car)
library(dplyr)
library(DT)
library(glmnet)
library(ggplot2)
library(latex2exp)
library(purrr)
library(rlang)
library(scales)
library(stringr)
library(tidyr)

source('descriptions.R')
source('encode.R')
```

# Summary

The work develops two models for predicting the sale price of houses in the
[Ames Housing Dataset](http://www.amstat.org/publications/jse/v19n3/decock.pdf):

  - A multilinear model that aims to give insight into the housing market
    represented by the dataset
  - A tree-based model that aims to improve on the accuracy of the multilinear
    model

## Primary variables of the multilinear model

The most important variables for the multilinear model are GrLivArea,
Functional, Condition1, MSZoning, and SaleCondition.

<div class="tab-display">

<div class="code-block">

```{r, summary, cache=TRUE, echo=FALSE}
report_summary <- function(importance_vec) {
    for (variable_name in names(importance_vec)) {
        cat(variable_descriptions[[variable_name]],
            '\n\nRelative importance: ',
            round(importance_vec[variable_name] * 100, digits = 1),
            '%',
            sep = '')
        if (!is.null(comments[[variable_name]])) {
           cat('\nComment: ', comments[[variable_name]],
               sep = '')
        }
        cat('\n\n\n')
    }
}
first_importance <- c(
    GrLivArea = 1.0000000,
    Functional = 0.7190126,
    Condition1 = 0.5192792,
    MSZoning = 0.3797086,
    SaleCondition = 0.3682316
)
report_summary(first_importance)
```

</div>

</div>

# Data preparation {.tabset}

## Summary {.unnumbered}

After preprocessing that repairs some inconsistencies in the data, display the
count of missing values for individual variables.

## Discussion {.unnumbered}

From the variable descriptions provided in *data_description.txt*, "NA" is a
valid category for certain columns.

Consistency checks revealed that "NA" sometimes indicates missing data, even
in variables where it represents a valid category.

  - Example:  The description of variable BsmtExposure has "NA" indicating "No
    Basement".  But the house with Id 949 has "NA" for BsmtExposure in spite
    of the fact that BsmtQual is "Good", BsmtCond is "Typical", and
    TotalBsmtSF ("Total square feet of basement area") is 936.  For this house,
    the "NA" for BsmtExposure likely indicates a missing value rather than "No
    Basement."

The script *preprocess.R* attempts to repair inconsistencies and replace "NA"
by a string such as "NB" for "No Basement" in cases where it represented a
valid category.

  - In the preprocessed data, only the variable LotFrontage ("Linear feet of
    street connected to property") has a significant number of missing values.

## Missing value counts {.tab-display .unnumbered}

<div class="code-block">

```{r, missing-data, cache=TRUE}
read_data <- function(file) {
    data <- read.csv(file, stringsAsFactors = FALSE)
    return(data)
}
train_data <- read_data('data/train.csv')
test_data <- read_data('data/test.csv')
get_na_counts <- function(data, dataset_name) {
    na_counts <- data.frame(name = colnames(data),
                            count = colSums(is.na(data)),
                            dataset = dataset_name,
                            stringsAsFactors = FALSE) %>%
        arrange(desc(count)) %>%
        filter(count != 0)
    return(na_counts)
}

na_counts_train <- get_na_counts(train_data, 'train')
na_counts_test <- get_na_counts(test_data, 'test')
combined_na_counts <- rbind(na_counts_test, na_counts_train,
                            make.row.names = FALSE) %>%
    arrange(desc(count), desc(dataset), name) %>%
    rename(`missing value count` = count)

# The same (complicated) set of options can be used with DT::datatable
# throughout this file.  A function wrapper is defined for this.
get_datatable <- function(data, page_length = 20) {
    table_alignment <- list(className = 'dt-center', targets = '_all')
    table_with_options <- datatable(
        data,
        rownames = FALSE,
        options = list(
            columnDefs = list(table_alignment),
            pageLength = page_length
         )
    )
    return(table_with_options)
}
get_datatable(combined_na_counts)
```

</div>

# Feature engineering, multilinear model

The goal is to define simple features that have a strong linear association
with the target.

Feature engineering was done in an iterative process.

  - Individual predictor variables that could explain variance in the target
    were plotted against the target.
  - Variables were transformed and/or new variables were defined to try to
    enhance linear associations visible in the plots.

## Variable distributions {.tabset}

### Summary {.unnumbered}

Apply transformations and plot the distribution of each variable.

### Discussion {.unnumbered}

The distribution of each variable is plotted.

  - Numeric predictors with only a few values are labeled as discrete, while
    those with many values are labeled as continuous.
  - For variables that were transformed, the distributions before and after the
    transformation are both shown, along with the formula for the
    transformation.
  - The variable descriptions from *data_description.txt* are included with the
    plots.

Transformation of the target variable and the predictor variables GrLivArea,
LotArea decreased skew.

Several variables have a distribution concentrated into a single value or a
small subset of the values.

  - For example, most houses have zero pool area.

### Target {.tab-display .unnumbered}

<div class="code-block">

<!-- R chunk to keep echoed code in one continuous block -->

```{r, show-plot-target, ref.label='plot-target', eval=FALSE}
```

```{r, plot-target, cache=TRUE, echo=FALSE}
to_plot <- select(train_data, SalePrice) %>%
    mutate(SalePrice = SalePrice / 1000)
fig <- ggplot(to_plot,
              aes(x = SalePrice)) +
    geom_histogram(fill = 'navyblue', bins = 30) +
    scale_x_continuous(name = 'SalePrice',
                       labels = label_dollar(suffix = 'k')) +
    ggtitle('SalePrice')
print(fig)

to_plot <- mutate(to_plot, SalePrice = log(SalePrice))
fig <- ggplot(select(to_plot, SalePrice),
              aes(x = SalePrice)) +
    geom_histogram(fill = 'navyblue', bins = 30) +
    scale_x_continuous(name = 'log(SalePrice)') +
    ggtitle('log(SalePrice)')
print(fig)
cat('transformation:  price -> log(price)\n\n')
```

</div>

### Discrete predictors {.tab-display .unnumbered}

<div class="code-block">

```{r, setup-distributions, cache=TRUE, echo=FALSE}
# Given a vector of variable names, reorder the names to match the order used in
# the original data set (i.e., in csv files and data_description.txt).
all_column_names <- colnames(train_data)
reorder_names <- function(variable_names) {
    reorder_index <- order(match(variable_names, all_column_names))
    variable_names <- variable_names[reorder_index]
    return(variable_names)
}

# For numeric variables with only a few values (such as YrSold), it is not
# initially clear whether the variable should be treated as categorical or
# numeric in, say, a linear model.  For plots, however, it is natural to treat
# these variables as categorical initially.
#
# Define three classes of variables:  categorical, continuous, and discrete
# numeric.  Here 'continuous' is used for numeric variables containing many
# values.

# Note that MSSubClass is a nominal variable labeled with nonconsecutive integer
# values, and it should be treated as categorical.
train_data <- mutate(train_data, MSSubClass = as.factor(MSSubClass))
categorical_predictors <- reorder_names(colnames(
    select(train_data, where(is.character), MSSubClass)
))

discrete_numeric <- reorder_names(c(
    'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
    'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',
    'GarageCars', 'MoSold', 'YrSold'
))

continuous_predictors <- reorder_names(setdiff(
    all_column_names,
    c(categorical_predictors, discrete_numeric, 'Id', 'SalePrice')
))

# Helper function that finds the total number of characters needed for the
# labels on the horizontal axis.  If this number is too large, the labels are
# rotated.
nchar_labels <- function(vector_to_plot) {
    # Drop missing values.
    vector_to_plot <- vector_to_plot[!is.na(vector_to_plot)]
    char_vector <- as.character(vector_to_plot)
    label_lengths <- nchar(unique(char_vector))
    return(sum(label_lengths))
}

# Helper function that rotates the labels of the horizontal axis if necessary.
set_label_angle <- function(fig, vector_to_plot) {
    if (nchar_labels(vector_to_plot) > 60) {
        fig <- fig + theme(
            axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
        )
    }
    return(fig)
}

# Helper function used in setting x-axis labels.
get_discrete_values <- function(vector_to_plot) {
    axis_labels <- seq(min(vector_to_plot),
                       max(vector_to_plot))
    return(axis_labels)
}

```

<!--
In this document, R chunks with labels of the form show-... use the ref.label
option to display the code for two chunks together as one block of code.

During development it is convenient to set cache=TRUE for the chunks that
generate many plots, while keeping cache=FALSE for chunks that have side
effects needed later in the document.  However, in the html output with folded
code, it is distracting to have the echoed code in two blocks, each with its
own 'Hide' or 'Show' button.  Using ref.label in a chunk with eval=FALSE
allows the code from two chunks to be displayed as one block of code.
-->

```{r, show-discrete, ref.label=c('setup-distributions', 'discrete-distributions'), eval=FALSE}
```

```{r, discrete-distributions, cache=TRUE, echo=FALSE}
to_plot_categorical <- reorder_names(
    c(categorical_predictors, discrete_numeric)
)
for (variable_name in to_plot_categorical) {
    # The list variable_descriptions is defined in 'descriptions.R'
    description <- variable_descriptions[[variable_name]]
    to_plot <- select(train_data, all_of(variable_name)) %>%
        drop_na()

    # In cases where the variable is an integer, converting it to a factor
    # before plotting decreases the amount of tweaking that is needed for the
    # plot.  However, there may be missing values in the horizontal axis.  In
    # the training set, for instance, there are no samples that have 7
    # bedrooms above ground, so 6 ends up next to 8 without special handling.
    # In order to avoid this, use the limits argument of scale_x_discrete.
    is_int <- is.integer(to_plot[[variable_name]])
    if (is_int) {
        x_axis_labels <- get_discrete_values(to_plot[[variable_name]]) %>%
            as.character
        to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]])
    }

    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_bar(fill = 'navyblue') +
        ggtitle(variable_name)
    if (is_int) {
        fig <- fig + scale_x_discrete(limits = x_axis_labels)
    } else {
        fig <- set_label_angle(fig, to_plot[[variable_name]])
    }
    print(fig)
    cat(description)
}
```

</div>

### Continuous predictors {.tab-display .unnumbered}

<div class="code-block">

```{r, transform, cache=TRUE, echo=FALSE}
# From the consistency checks done by preprocess.R, there are no missing
# values for GarageYrBlt in eiether the training data or the test data.  The
# preprocessing script set this variable to zero for houses without a garage.
# For convenience in dropping data for plotting, temporarily replace zero by
# NA.
bool_index <- train_data$GarageYrBlt == 0
train_data[bool_index, 'GarageYrBlt'] <- NA

# Function used for transforming YearBuilt and GarageYrBlt
transform_year <- function(x) {
    return((x - 1900)^3 / 1e6)
}
transform_year_formula <- 'year -> (year - 1900)^3 / 1e6'
transform_area_formula <- 'area -> log(area)'

# The information associated with transforming variables is entered as a list
# of lists.  In order to avoid awkward syntax in accessing elements of nested
# list, however, the nested list is used to generated simple 1D lists and
# vectors.  (The reason for using the nested list initially is to group
# together the information for a particular variable.)
transformations <- list(
    GrLivArea = list(func = log,
                     label = 'log(GrLivArea)',
                     formula = transform_area_formula),
    YearBuilt = list(func = transform_year,
                     label = 'transformed YearBuilt',
                     formula = transform_year_formula),
    GarageYrBlt = list(func = transform_year,
                       label = 'transformed GarageYrBlt',
                       formula = transform_year_formula),
    LotArea = list(func = log,
                   label = 'log(LotArea)',
                   formula = transform_area_formula)
)
transformed_variables <- names(transformations)
transformed_variable_labels <- map_chr(transformations, ~ (.)$label)
transformation_functions <- map(transformations, ~ (.)$func)
transformation_formulas <- map_chr(transformations, ~ (.)$formula)

get_variable_label <- function(variable_name) {
    if (variable_name %in% transformed_variables) {
        variable_label <- transformed_variable_labels[variable_name]
    } else {
        variable_label <- variable_name
    }
    return(variable_label)
}

```

```{r, show-transform, ref.label=c('transform', 'continuous-distributions'), eval=FALSE}
```


```{r, continuous-distributions, cache=TRUE, echo=FALSE}
for (variable_name in continuous_predictors) {
    description <- variable_descriptions[[variable_name]]
    to_plot <- select(train_data, all_of(variable_name)) %>%
        drop_na()

    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_histogram(fill = 'navyblue', bins = 30) +
        ggtitle(variable_name)
    print(fig)

    if (variable_name %in% transformed_variables) {
        # Apply the transformation to the variable.
        func <- transformation_functions[[variable_name]]
        formula <- transformation_formulas[variable_name]
        to_plot[[variable_name]] <- func(
            to_plot[[variable_name]]
        )

        variable_label <- get_variable_label(variable_name)
        fig <- ggplot(to_plot,
                      aes_string(x = variable_name)) +
            geom_histogram(fill = 'navyblue', bins = 30) +
            scale_x_continuous(name = variable_label) +
            ggtitle(variable_label)
        print(fig)

        cat('transformation: ', formula, '\n\n')
    }

    cat(description)
}
```

</div>

## Which predictors can explain the variance in the sale price? {.tabset}

### Summary {.unnumbered}

Show the fraction of variance in the sale price explained by individual
predictors.

### Discussion {.unnumbered}

ANOVA ($\eta^2$) is used for nominal predictors, and $R^2$ is used for numeric
predictors.

For many of the predictors that individually explain the largest fraction of
variance, the value of $\eta^2$ or $R^2$ was increased by the transformations.

  - Example:  the highest value of $R^2$ (OverallQual) increased from 0.63 to
    0.67 due to the transformation of SalePrice.

### Nominal predictors {.tab-display .unnumbered}

<div class="code-block">

```{r, eta-squared, cache=TRUE}
# Apply transformations to variables in the training data.  The R chunks that
# generate large plots are cached, and so transformations in those chunks
# were done to temporary plotting data frames.
apply_transformations <- function(data) {
    if ('SalePrice' %in% colnames(data)) {
        data <- mutate(data, SalePrice = log(SalePrice))
    }

    for (variable_name in transformed_variables) {
        # Apply the transformation to the variable.
        func <- transformation_functions[[variable_name]]
        data[[variable_name]] <- func(data[[variable_name]])
    }

    return(data)
}

train_data <- apply_transformations(train_data)

get_linear_model <- function(variable_name,
                             dummy_filter = '',
                             outliers = integer()) {
    # For categorical variables, perform ANOVA and return eta squared along
    # with predictions based on the model.  For numeric variables, do a simple
    # linear regression and return R squared along with predictions.
    #
    # Note that the command aov used for ANOVA in R is a wrapper to lm that
    # provides a summary in the traditional ANOVA form.  This summary doesn't
    # include eta squared.  However, the value is available as r.squared from
    # the summary for lm.  So if aov is used to generate the model, then eta
    # squared is available as summary.lm(model)$r.squared.
    #
    # In the current function, use the lm command both for categorical and
    # numeric predictors.
    data <- filter(train_data, !(Id %in% outliers))
    model_formula <- paste0('SalePrice ~ ', variable_name)
    if  (nchar(dummy_filter) > 0) {
        model_formula <- paste(model_formula, '+', dummy_filter)
        vars <- c(variable_name, dummy_filter)
        data <- select(data, SalePrice, all_of(vars)) %>%
            drop_na()
    } else {
        data <- select(data, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }
    model_formula <- as.formula(model_formula)
    model <-  lm(model_formula, data)
    model$formula <- model_formula
    model$dummy_filter <- dummy_filter
    model$outliers <- outliers
    return(model)
}

anova_models <- map(categorical_predictors, get_linear_model)
names(anova_models) <- categorical_predictors
eta_squared_vec <- map_dbl(anova_models, ~ summary(.)$r.squared)
eta_squared_df <- data.frame(name = categorical_predictors,
                             eta.squared = eta_squared_vec,
                             stringsAsFactors = FALSE) %>%
    arrange(desc(eta.squared)) %>%
    rename(`eta squared` = eta.squared)
get_datatable(eta_squared_df) %>%
    formatRound(columns = 'eta squared', digits = 3)
```

</div>

### Numeric predictors  {.tab-display .unnumbered}

<div class="code-block">

```{r, r-squared, cache=TRUE}
numeric_predictors <- reorder_names(
    c(continuous_predictors, discrete_numeric)
)

linear_models <- map(numeric_predictors, get_linear_model)
names(linear_models) <- numeric_predictors
r_squared_vec <- map_dbl(linear_models, ~ summary(.)$r.squared)
r_squared_df <- data.frame(name = numeric_predictors,
                           r.squared = r_squared_vec,
                           stringsAsFactors = FALSE) %>%
    arrange(desc(r.squared)) %>%
    rename(`R squared` = r.squared)
get_datatable(r_squared_df) %>%
    formatRound(columns = 'R squared', digits = 3)
```

</div>

## Plots of sale price vs individual predictors {#association-plots .tabset}

### Summary {.unnumbered}

Visualize the dependence of sale price on individual predictors.

### Discussion {.unnumbered}

Predictors with large $\eta^2$ or $R^2$ are plotted against sale price.

  - Here "large" is defined to mean $\gtrsim 0.1$.
  - For both $\eta^2$ and $R^2$, there is a gap in the values at around 0.1, and
    predictors falling above the gap are plotted.
  - Predictor distributions and descriptions are also shown.

The linear profile of several of the predictors improved dramatically due to
the transformations.

  - For example, the transformation caused the median values shown in many of
    the box plots to fall roughly on a line.


### Box plots {.tab-display .unnumbered}

<div class="code-block">

```{r, large-eta, cache=TRUE, echo=FALSE}
large_eta_squared <- filter(eta_squared_df, `eta squared` >= 0.085)

generate_box_plot <- function(data, variable_name, eta_squared = NULL,
                              show_encoding = FALSE) {

    description <- variable_descriptions[[variable_name]]
    if (show_encoding) {
        encoded_variable_name <- paste0(variable_name, 'Num')
        vars <- c(variable_name, encoded_variable_name)
        to_plot <- select(data, SalePrice, all_of(vars)) %>%
            rename(encoding = .data[[encoded_variable_name]]) %>%
            drop_na()
    } else {
        to_plot <- select(data, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }
    to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]]) %>%
        reorder(to_plot$SalePrice, median)

    if (!is.null(eta_squared)) {
        plot_title <- paste0(variable_name, ', ', '$\\eta^2 = ',
                             eta_squared, '$')
    } else {
        plot_title <- variable_name
    }
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_boxplot(aes_string(y = 'SalePrice', fill = variable_name))
    if (show_encoding) {
        fig <- fig +
            geom_point(aes(y = encoding, color = 'encodings')) +
            scale_color_manual(name = variable_name,
                               values = c('encodings' = 'red')) +
            guides(fill = FALSE)
    }
    fig <- fig + scale_y_continuous(name = 'log(SalePrice)') +
        scale_x_discrete(name = variable_name) +
        ggtitle(TeX(plot_title))
    fig <- set_label_angle(fig, to_plot[[variable_name]])
    print(fig)

    # Bar chart of the distribution
    plot_title <- paste(variable_name, 'distribution')
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_bar(fill = 'navyblue') +
        scale_x_discrete(name = variable_name) +
        ggtitle(plot_title)
    fig <- set_label_angle(fig, to_plot[[variable_name]])
    print(fig)

    cat(description)
}
```

```{r, show-large-eta, ref.label=c('large-eta', 'box-plots'), eval=FALSE}
```

```{r, box-plots, cache=TRUE, echo=FALSE}
for (variable_name in large_eta_squared$name) {
    eta_squared <- round(eta_squared_vec[variable_name], digits = 3)
    generate_box_plot(train_data, variable_name, eta_squared)
}
```

</div>

### Scatter plots {.tab-display .unnumbered}

<div class="code-block">

```{r, large-r, cache=TRUE, echo=FALSE}
large_r_squared <- filter(r_squared_df, `R squared` >= 0.095)

# Reusable plotting function
plot_simple_fit <- function(variable_name, linear_model, r_squared = 0) {
    outliers <- linear_model$outliers
    dummy_filter <- linear_model$dummy_filter
    description <- variable_descriptions[[variable_name]]
    to_plot <- filter(train_data, !(Id %in% outliers))

    if (nchar(dummy_filter) > 0) {
        to_plot <- filter(to_plot, .data[[dummy_filter]] == 0) %>%
            select(SalePrice, all_of(c(variable_name, dummy_filter))) %>%
            drop_na()
    } else {
        to_plot <-  select(to_plot, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }

    prediction <- predict(linear_model, to_plot)

    to_plot <- mutate(to_plot, PredictedPrice = prediction)

    # Scatter plot
    variable_label <- get_variable_label(variable_name)
    if (as.logical(r_squared)) {
        plot_title <- paste0(
            variable_label, ', ', '$R^2 = ', r_squared, '$'
        )
    } else {
        plot_title <- variable_label
    }
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_point(aes(y = SalePrice, color = 'data')) +
        geom_line(aes(y = PredictedPrice, color = 'fit')) +
        scale_color_manual(
            name = NULL,
            values = c(
                'data' = 'navyblue',
                'fit' = 'red'
            )) +
        scale_y_continuous(name = 'log(SalePrice)') +
        ggtitle(TeX(plot_title))

    is_discrete <- variable_name %in% discrete_numeric
    if (is_discrete) {
        breaks <- get_discrete_values(to_plot[[variable_name]])
        labels <- as.character(breaks)
        fig <- fig + scale_x_continuous(
            name = variable_label, breaks = breaks, labels = labels
        )
    } else {
        fig <- fig + scale_x_continuous(name = variable_name)
    }
    print(fig)

    # Histogram or bar chart of the distribution
    plot_title <- paste(variable_label, 'distribution')
    if (is_discrete) {
        to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]])
        fig <- ggplot(to_plot, aes_string(x = variable_name)) +
            geom_bar(fill = 'navyblue') +
            scale_x_discrete(limits = labels)
    } else {
        fig <- ggplot(to_plot, aes_string(x = variable_name)) +
            geom_histogram(fill = 'navyblue', bins = 30) +
            scale_x_continuous(name = variable_label)
    }
    print(fig + ggtitle(plot_title))

    if (variable_name %in% transformed_variables) {
        cat('transformation: ',
            transformation_formulas[variable_name],
            '\n\n'
        )
    }
    cat(description)
}

```

```{r, show-large-r, ref.label=c('large-r', 'scatter-plots'), eval=FALSE}
```

```{r, scatter-plots, cache=TRUE, echo=FALSE}
for (variable_name in large_r_squared$name) {
    r_squared <- round(r_squared_vec[variable_name], digits = 3)
    linear_model <- linear_models[[variable_name]]
    plot_simple_fit(variable_name, linear_model, r_squared)
}
```

</div>

## Define dummy variables, exclude outliers {.tabset}

### Summary {.unnumbered}

Enhance the linear dependence of sale price on individual numeric predictors.

### Discussion {.unnumbered}

The starting point for the process is the set of predictors that have large
$R^2$.

For certain scatter plots, the fit appears distorted by outliers and/or by
the points at the vertical intercept.

Distortions due to the vertical intercept can be eliminated by defining a
dummy variable.

  - Example:  A dummy variable NoBasement that identifies houses without a
    basement was defined in order to avoid distortions in the fit for
    variables that characterize basement area.

With outliers excluded and appropriate dummy variables defined, $R^2$
increased and the fit shown in scatter plots improved.


### Changes in $R^2$ {.tab-display .unnumbered}

<div class="code-block">

```{r, dummy-variables, cache=TRUE, echo=FALSE}
# Add dummy variables to data.  From the consistency checks done by the
# preprocessing script, the variables used in defining NoBasement,
# NoGarage, NoFireplace had no missing values in the original data sets
# (both training and test).  However, the current script set houses with no
# garage to have GarageYr to NA to facilitate plotting, and so is.na is used
# to define the variable NoGarage.
define_dummies <- function(data) {
    data <- mutate(data,
                   NoBasement = as.integer(TotalBsmtSF == 0),
                   NoGarage = as.integer(is.na(GarageYrBlt)),
                   NoFireplace = as.integer(Fireplaces == 0),
                   NoSecondFloor = as.integer(SecondFlrSF == 0))
    return(data)
}
train_data <- define_dummies(train_data)

# Now that NoGarage can be used to distinguish houses without a garage,
# GarageYrBlt can be set to an arbitrary value for houses without a garage.
# Choose -0.1 as an arbitrary value to separate these houses in the plot of
# GarageYrBlt.
bool_index <- is.na(train_data$GarageYrBlt)
train_data[bool_index, 'GarageYrBlt'] <- -0.1

basement_related <- c(
    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',
    'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',
    'BsmtHalfBath'
)
garage_related <- c(
    'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'GarageYrBlt',
    'GarageCars', 'GarageArea'
)
fireplace_related <- c('FireplaceQu', 'Fireplaces')
second_floor_related <- 'SecondFlrSF'

# Find the dummy variable that imposes a constraint on the values for a
# variable.
get_dummy_filter <- function(variable_name) {
    dummy <- case_when(
        variable_name %in% basement_related ~ 'NoBasement',
        variable_name %in% garage_related ~ 'NoGarage',
        variable_name %in% fireplace_related ~ 'NoFireplace',
        variable_name %in% second_floor_related ~ 'NoSecondFloor',
        TRUE ~ ''
    )
    return(dummy)
}

bool_index <- !(all_column_names %in% c('Id', 'SalePrice'))
original_predictors <- all_column_names[bool_index]
dummy_filters <- get_dummy_filter(original_predictors)
names(dummy_filters) <- original_predictors

```

```{r, outliers, cache=TRUE, echo=FALSE}
# For certain variables, the fit is visibly off because of a few outliers for
# which the variable has a very large value.  Define a vector of formula
# strings to use for dropping outliers in evaluating the fit of individual
# variables.
outlier_formulas <- c(
    GrLivArea = '>= log(4500)',
    GarageArea = '>= 1240',
    TotalBsmtSF = '>= 3000',
    BsmtFinSF1 = '>= 3000',
    SecondFlrSF = '> 3000',
    LotFrontage = '> 300'
)

# Convert these formulas into a list of outliers.
get_outliers <- function(data, variable_name) {
    if (variable_name %in% names(outlier_formulas)) {
        formula <- parse_expr(paste(
            variable_name, outlier_formulas[[variable_name]]
        ))
        filtered_data <- filter(data, !!formula)
        outliers <- filtered_data$Id
    }
    else {
        outliers <- integer()
    }
    return(outliers)
}

variable_outliers <- map(original_predictors,
                         ~ get_outliers(train_data, .))
names(variable_outliers) <- original_predictors
```

```{r, r-squared-changes, ref.label=c('dummy-variables', 'outliers', 'update-r-squared'), eval=FALSE}
```

```{r, update-r-squared, cache=TRUE, echo=FALSE}
# In order to check how linear fits are improved by the exclusion of outliers
# and/or the addition of dummy variables, generate new linear models.  For
# example, include NoBasement as a second predictor in the finding the
# linear fit for the predictor TotalBsmtSF, and exclude
# variable_outliers[['TotalBsmtSF']] from the fit.
#
# As new fits are generated, append values to vectors that will form a data
# frame showing changes in R squared.
original_model <- character()
updated_model <- character()
original_r_squared <- numeric()
updated_r_squared <- numeric()
for (variable_name in large_r_squared$name) {
    outliers <- variable_outliers[[variable_name]]
    dummy_filter <- dummy_filters[variable_name]
    model_update_needed <- length(outliers) > 0 | nchar(dummy_filter) > 0
    if (model_update_needed) {
        # The elements of dummy_filters are named.  For instance, in the case
        # where variable_name is 'TotalBsmtSF', the variable
        #
        # dummy_filter <- dummy_filters[variable_name]
        #
        # is the following:
        #
        # > dummy_filter
        #  TotalBsmtSF
        #  "NoBasement"
        #
        # If dummy_filter is used in a dplyr select statement, the column
        # 'NoBasement' gets selected and then renamed to TotalBsmtSF.  In
        # order to avoid this, set the name of dummy_filter to NULL.
        names(dummy_filter) <- NULL
        linear_model <- get_linear_model(
            variable_name, dummy_filter, outliers = outliers
        )
        linear_models[[variable_name]] <- linear_model

        original_model <- c(original_model, variable_name)
        if (nchar(dummy_filter) > 0) {
            updated_model <- c(updated_model,
                               paste(variable_name, '+', dummy_filter))
        } else {
            updated_model <- c(updated_model, variable_name)
        }
        original_r_squared <- c(original_r_squared,
                                r_squared_vec[variable_name])
        updated_r_squared <- c(updated_r_squared,
                               summary(linear_model)$r.squared)
    }
}

r_squared_changes <- data.frame(`original model` = original_model,
                                `original R squared` = original_r_squared,
                                `updated model` = updated_model,
                                `updated R squared` = updated_r_squared,
                                check.names = FALSE,
                                stringsAsFactors = FALSE)
get_datatable(r_squared_changes) %>%
    formatRound(columns = c('original R squared', 'updated R squared'),
                digits = 3)
```

</div>

### Scatter plots {.tab-display .unnumbered}

<div class="code-block">

```{r, better-scatter-plots, cache=TRUE}
for (variable_name in r_squared_changes[['original model']]) {
    bool_index <- r_squared_changes[['original model']] == variable_name
    r_squared <- round(r_squared_changes[bool_index, 'updated R squared'],
                       digits = 3)
    linear_model <- linear_models[[variable_name]]
    plot_simple_fit(variable_name, linear_model, r_squared)
}
```

</div>

## Encode nominal variables with numbers {.tabset}

### Summary {.unnumbered}

Apply $k$-fold target encoding to each nominal variable.  For example, encoding
of the Neighborhood variable yielded a NeighborhoodNum variable.

### Discussion {.unnumbered}

Target encoding encodes each category by the mean of the target values for
that category.

This method is prone to overfitting, but variants have been developed to
minimize overfitting:

  - $K$-fold target encoding randomly separates the training data into $k$
    folds.  Samples in a given fold are encoded using data not in that fold.
  - For categories with a small number of samples, the encoding is a mix of
    the category target mean and the global target mean.

Five folds were used here, and the weighting of the global target mean was
given by a sigmoid function that decreased from $\approx 1$ to $\approx 0$ as
the number of samples in the category increased from 0 to 10.

The script *encode.R* created for this project supports two methods of encoding
test data:

  - Given an observation and the class for that observation, randomly select one
    of the k encodings generated for the class.

  - All observations of a given class receive the mean of the k encodings
    generated for the class.  This is the natural choice for a linear model,
    since it is equivalent to averaging the model output over many possible
    random encodings of the predictors.

Missing values were encoded using the global target mean.


### Visualize encodings {.tab-display .unnumbered}

<div class="code-block">

```{r, target-encode, cache=TRUE, echo=FALSE}
# Encode all categorical predictors.

# Convert MSSubClass to a character vector to simplify the encoding.  Note
# that 'MSSubClass' is a factor vector, while the rest of the categorical
# predictors are character vectors.  The reason that MSSubClass is handled
# differently is that the variable is a scattered set of integers that
# represent classes.  If this variable is converted to a character early in
# the current script, the ordering of factors for plots is lexical, which
# '190' coming before '20', which makes the plotted distribution of MSSubClass
# harder to understand.  Converting it from an integer vector (imported by
# read.csv) directly to a factor automatically gives factors ordered by
# integer value in the distribution plot.
train_data <- mutate(train_data, MSSubClass = as.character(MSSubClass))

target_mean <- mean(train_data$SalePrice)

encode_predictors <- function(data, method = 'mean', drop_categorical = TRUE) {
    # The functions encode and assign_encoding are defined in the script
    # encode.R.
    if ('SalePrice' %in% colnames(data)) {
        # Encode training data.
        for (variable_name in categorical_predictors) {
            # The encode function is defined in the script encode.R.
            data <- encode(data, variable_name, target_mean)
        }
    } else {
        # Assign encodings to test data.
        for (variable_name in categorical_predictors) {
            data <- assign_encoding(data, variable_name,
                                    target_mean = target_mean, method = method)
        }
    }

    if (drop_categorical) {
        data <- select(data, -all_of(categorical_predictors))
    }

    return(data)
}

train_data <- encode_predictors(train_data, drop_categorical = FALSE)
```

```{r show-encoded, ref.label=c('target-encode', 'check-encoding'), eval=FALSE}
```

```{r, check-encoding, cache=TRUE, echo=FALSE}
for (variable_name in large_eta_squared$name) {
    eta_squared <- round(eta_squared_vec[variable_name], digits = 3)
    generate_box_plot(train_data, variable_name, eta_squared,
                      show_encoding = TRUE)
}
```

</div>

## Outcome of feature engineering {.tabset}

### Summary {.unnumbered}

List the changes applied to variables in the original data set.

### Details {.unnumbered}

Changes to the variables:

  1. The target (SalePrice) and two predictors (LotArea, GrLivArea) were
     transformed using the logarithm.
  2. The predictors YearBuilt and GarageYrBlt were transformed using the
     function $f(x) = (x - 1900)^3$.
  3. Dummy variables NoBasement, NoGarage, NoFireplace, NoSecondFloor were
     defined in order to eliminate distortions in the fit for related numeric
     variables.
  4. Outliers for individual numeric predictors were defined.
  5. Each nominal variable in the training set was encoded using $k$-fold
     target encoding with $k=5$.  For categories with $<10$ samples, a sigmoid
     function mixed the global target mean with the category target mean to
     give the encoding.  As a natural extension of this step, missing
     categorical data was encoded with the global target mean.

## Additional possibilities for feature engineering {.tabset}

### Summary {.unnumbered}

Discuss additional feature engineering that could lead to an improved
multilinear model.

### Discussion {.unnumbered}

New variables could be defined by eliminating poorly sampled discrete values.

  - Example:  There are only a few houses in the training data with GarageCars
    equal to 4, and these houses all have a price lower than that predicted by
    the linear fit.  The model would likely be improved by merging the values
    3 and 4 in GarageCares, i.e., using the value 3 to represent all houses
    with 3 or more cars.

Methods of encoding categorical variables could be more fully explored using
cross validation for a particular model.

  - Example:  For k-fold target encoding, cross-validation could be used to
    tune the number of folds as well as the weight given to the global target
    mean for categories with a small number of samples.

## Are the predictors independent? {.tabset}

### Summary {.unnumbered}

Display a table of correlations between predictors as an aid to tuning the
multilinear model.

### Discussion {.unnumbered}

A correlation matrix was calculated that included the following 45 predictors:

  - Numeric predictors with large $R^2$.
  - Encoded versions of categorical predictors with large $\eta^2$.

In the table, correlations are presented in order of decreasing magnitude.

Many of the predictors that have a strong linear association with the target
are also strongly correlated with other predictors.

### Correlations {.tab-display .unnumbered}

<div class="code-block">

```{r, correlations, cache=TRUE}
linear_predictors <- c(large_r_squared$name,
                       paste0(large_eta_squared$name, 'Num'))

corr_matrix <- select(train_data, all_of(linear_predictors)) %>%
    cor(use = 'pairwise.complete.obs')
# Set to NA the upper triangular part of the correlation matrix.
bool_index <- lower.tri(corr_matrix, diag = TRUE)
corr_matrix[bool_index] <- NA

correlations <- as.data.frame(corr_matrix) %>%
    mutate(var_1 = rownames(.)) %>%
    pivot_longer(-var_1, names_to = 'var_2', values_to = 'corr') %>%
    filter(!is.na(corr)) %>%
    arrange(desc(abs(corr))) %>%
    rename(`predictor 1` = var_1, `predictor 2` = var_2,
           correlation = corr)
get_datatable(correlations) %>%
    formatRound(columns = 'correlation', digits = 3)
```

 </div>

# Multilinear model

A descriptive model that aims to give insight into the housing market.

## Stepwise regression {.tabset}

### Summary {.unnumbered}

Use stepwise regression to select variables for the model.

### Discussion {.unnumbered}

An iterative process was used to tune the stepwise regression:

  1. Exclude outliers after inspecting previous results with plot.lm.  Three
     outliers were dropped in the final iteration.

  2. Drop variables that contain many missing values.  In the first iteration,
     only LotFrontage was dropped, but tests found that MasVnrArea was not an
     important variable at any step of the regression, and so it was dropped as
     well.   The remaining numeric and encoded categorical predictors had no
     missing values.

  3. Execute the stepwise regression using a data set that includes all
     numeric and encoded categorical predictors, together with dummy variables
     such as NoBasement.  Use the Bayesian information criterion in order to
     favor model simplicity.

  4. Inspect the model selected by stepwise regression, particularly checking
     the variance inflation factors and the effects of outliers.

$K$-fold target encoding introduces a random element into the training process,
and as a result, the set of variables selected by stepwise regressions varied
with successive iterations of the training process.

  - Stepwise regression consistently yielded around 27 variables, but two or
    three of the selected variables would typically change between iterations.

After the process was tuned, a handful of repetitions of the stepwise selection
were performed, with a log generated for each repetition.  One of the
repetitions gave a somewhat lower variance inflation factor than the others, and
the variables selected in this repetition were used as the starting point for
lasso regression.

### Outcome {.tab-display .unnumbered}

<div class="code-block">

```{r show-lm, ref.label=c('stepwise-regression'), eval=FALSE}
```

```{r, stepwise-regression, cache=TRUE, echo=FALSE}
outliers <- c(1299, 524, 633)
dummy_variables <- c('NoBasement', 'NoGarage', 'NoFireplace',
                     'NoSecondFloor')
encoded_categorical <- paste0(categorical_predictors, 'Num')
lm_predictors <- c(continuous_predictors, discrete_numeric,
                   encoded_categorical, dummy_variables)
# Drop LotFrontage and MasVnrArea because of missing values.
bool_index <- !(lm_predictors %in% c('LotFrontage', 'MasVnrArea'))
lm_predictors <- lm_predictors[bool_index]

# Boolean that determines whether stepwise regressions is run when the
# markdown file is rendered.
select_lm <- FALSE
if (select_lm) {
    # Note that if this code is executed as a single block, some of the output
    # that should be captured by the sink() command does not get captured.  In
    # practice, this code was executed interactively during an iterative process
    # of model development.  When commands are executed interactively while
    # waiting for one command to complete before executing the next, all output
    # is captured by the sink() command.

    # Because of the select command below, lm_train_data has no missing
    # values.
    lm_train_data <- train_data %>%
        select(all_of(lm_predictors), SalePrice, Id) %>%
        filter(!(Id %in% outliers))
    trivial_model <- lm(SalePrice ~ 1, data = lm_train_data)
    scope <- list(
        lower = formula(trivial_model),
        upper = as.formula(
            paste('SalePrice ~',
                  paste0(lm_predictors, collapse = ' + '))
        )
    )

    sink('stepwise_search_BIC.log')
    best_lm <- step(trivial_model, scope, direction = 'both',
                    k = log(nrow(lm_train_data)))
    print(summary(best_lm))
    best_lm_predictors <- names(best_lm$coefficients)[-1]
    cat('Predictors:\n\n')
    print(best_lm_predictors)
    cat('Variance inflation factors:\n\n')
    print(vif(best_lm))
    sink()
} else {
    best_lm_predictors <- c(
        'OverallQual', 'GrLivArea', 'NeighborhoodNum', 'BsmtFinSF1',
        'GarageArea', 'OverallCond', 'YearBuilt', 'TotalBsmtSF',
        'MSZoningNum', 'Fireplaces', 'LotArea', 'SaleConditionNum',
        'BldgTypeNum', 'Condition1Num', 'BsmtExposureNum', 'KitchenQualNum',
        'FunctionalNum', 'CentralAirNum', 'NoBasement', 'SecondFlrSF',
        'NoSecondFloor', 'ScreenPorch', 'GarageCondNum', 'WoodDeckSF',
        'HeatingQCNum', 'BsmtQualNum', 'BsmtFullBath'
    )
    stepwise_formula <- as.formula(
        paste('SalePrice ~',
              paste0(best_lm_predictors, collapse = ' + '))
    )
    lm_train_data <- train_data %>%
        select(all_of(best_lm_predictors), SalePrice, Id) %>%
        filter(!(Id %in% outliers))
    best_lm <- lm(stepwise_formula, lm_train_data)
    print(summary(best_lm))
    cat('Variance inflation factors:\n\n')
    print(vif(best_lm))
}

plot(best_lm)
```

</div>

## Lasso regression {.tabset}

### Summary {.unnumbered}

Fine tune the feature selection with lasso regression, and add regularization
to the multilinear model.

### Discussion {.unnumbered}

Cross validation with lasso regression was used to eliminate the variables
NoSecondFloor and SecondFlrSF, which significantly improved the variance
inflation factors for the model.

  - The command cv.glmnet was executed, and the coefficients at lambda.1se were
    checked for zeros.   Although there was some variation in the outcome
    between iterations, NoSecondFloor and SecondFlrSF were consistently set to
    zero by this process.  The choice to eliminate SecondFlrSF was supported by
    the fact it has a strong correlation with GrLivArea, probably the most
    important variable in the model.

After the feature selection was finalized, the cross validation was performed
again to find the optimal regularization.

  - During a handful of iterations of the training process, the optimal value of
    lambda was consistently 0.0007.

### Outcome {.tab-display .unnumbered}

<div class="code-block">

```{r show-lasso, ref.label=c('lasso'), eval=FALSE}
```

```{r, lasso, cache=TRUE, echo=FALSE}
get_glmnet_inputs <- function(data, predictors, outliers = NULL) {
    if (!is.null(outliers)) {
        data <- data %>%
            filter(!(Id %in% outliers))
    }

    predictor_matrix <- data %>%
        select(all_of(predictors)) %>%
        data.matrix()

    if ('SalePrice' %in% colnames(data)) {
        target <- data$SalePrice
    } else {
        target <- NULL
    }

    inputs <- list(
        x = predictor_matrix,
        y = target
    )
    return(inputs)
}

glmnet_inputs <- get_glmnet_inputs(train_data, best_lm_predictors,
                                   outliers = outliers)
lasso_models <- glmnet(x = glmnet_inputs$x, y = glmnet_inputs$y,
                       alpha = 1, family = 'gaussian')
plot(lasso_models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
cv_lasso_model <- cv.glmnet(x = glmnet_inputs$x, y = glmnet_inputs$y,
                            alpha = 1, family = 'gaussian',
                            nfolds = 10)
plot(cv_lasso_model, main = "Lasso Regression\n")
best_lambda <- cv_lasso_model$lambda.min

# Boolean that determins whether a test of feature selection is performed when
# the markdown file is rendered.
select_lasso_predictors <- FALSE
if (select_lasso_predictors) {
    # With lambda set to lambda.1se, two coefficients consistently go to zero.
    predict(lasso_models, s = cv_lasso_model$lambda.1se, type = 'coefficients')
} else {
    cat('Cross validation with lasso set the coefficients of SecondFlrSF and',
        'NoSecondFloor\nto zero.  The optimal value of lambda found by glmnet',
        'after these variables\nwere dropped was 0.0007.\n\n')

    best_formula <- update.formula(
        formula(best_lm),
        ~ . - NoSecondFloor - SecondFlrSF
    )
    best_lm <- lm(best_formula, lm_train_data)
    best_lm_predictors <- names(best_lm$coefficients)[-1]

    best_lambda <- 0.0007081519

    glmnet_inputs <- get_glmnet_inputs(train_data, best_lm_predictors,
                                       outliers = outliers)
    lasso_models <- glmnet(x = glmnet_inputs$x, y = glmnet_inputs$y,
                           alpha = 1, family = 'gaussian')

    print(summary(best_lm))
    cat('Variance inflation factors:\n\n')
    print(vif(best_lm))
    plot(best_lm)
}
```

</div>

## Variable importance and visualization {.tabset}

### Summary {.unnumbered}

Use the coefficients obtained with standardized input data to rank variables
by importance, and visualize the selected variables.

### Variable importance {.tab-display .unnumbered}

<div class="code-block">

```{r, importance-lm, cache=TRUE}
scaled_lm_train_data <- as.data.frame(cbind(
    scale(glmnet_inputs$x),
    SalePrice = glmnet_inputs$y
))

# Note that executing summary(scaled_lm_train_data) is a good way to check
# whether there are large outliers in any of the selected predictors.

lm_model_scaled <- lm(best_formula, lm_train_data)
standardized_coeffs <- sort(lm_model_scaled$coefficients[-1],
                            decreasing = TRUE)

lm_predictors_by_importance <- names(standardized_coeffs)

# Curly braces used to force output into a single block in the rendered
# markdown.
{
    cat('Predictors in decreasing order of importance, along with the ',
        'coefficients obtained\nusing standardized training data.\n\n')
    print(standardized_coeffs, digits = 3)
}
```

</div>

### Visualize variables {.tab-display .unnumbered}

<div class="code-block">

```{r, visualize-selected, cache=TRUE}
for (variable_name in lm_predictors_by_importance) {
    if (str_detect(variable_name, 'Num$')) {
        variable_name <- str_replace(variable_name, 'Num$', '')
        generate_box_plot(train_data, variable_name, show_encoding = TRUE)
    } else {
        dummy_filter <- get_dummy_filter(variable_name)
        # None of the dummy filters but NoBasement made it into the multilinear
        # model.
        if (dummy_filter != 'NoBasement') {
            dummy_filter <- ''
        }
        linear_model <- get_linear_model(variable_name, dummy_filter, outliers)
        plot_simple_fit(variable_name, linear_model)
    }
}
```

</div>

## Model metrics {.tabset}

### Summary {.unnumbered}

Compare error metrics for the training and test data.

### Metrics {.tab-display .unnumbered}

<div class="code-block">

```{r, show-linear-score, ref.label=c('linear-score'), eval=FALSE}
```

```{r, linear-score, cache=TRUE, echo=FALSE}
# During the process of target encoding, missing values in categorical
# variables are encoded with the mean of the target variable for the training
# data.
#
# Missing values in numeric variables will be imputed using means
# from the training data.  The function get_predictor_means returns a vector
# of stored means for the training data.  Note that the means are found for
# the 'raw' training data, rather than tranformed training data, since missing
# values in test data will be imputed before transformations are applied.
get_predictor_means <- function() {
    data <- read.csv('data/train.csv')
    means <- numeric(length(numeric_predictors))
    names(means) <- numeric_predictors
    for (variable_name in numeric_predictors) {
        means[variable_name] <- mean(
            data[[variable_name]],
            na.rm = TRUE
        )
    }
    return(means)
}

means <- get_predictor_means()
impute_vector <- function(vec, variable_name) {
    bool_index <- is.na(vec)
    vec[bool_index] <- means[variable_name]
    return(vec)
}
impute_data <- function(data, verbose = FALSE) {
    # Impute missing numerical values by the mean from the training data.
    for (variable_name in intersect(numeric_predictors, colnames(data))) {
        missing_count <- sum(is.na(data[[variable_name]]))
        if (missing_count > 0) {
            if (verbose) {
                cat('Imputing ', missing_count, ' missing value(s) in ',
                    variable_name, '.\n', sep = '')
            }
            data[[variable_name]] <- impute_vector(data[[variable_name]],
                                                   variable_name)
        }
    }

    return(data)
}

preprocess_lm <- function(data, drop_categorical = TRUE, verbose = FALSE) {
    data <- select(data, -LotFrontage, -MasVnrArea) %>%
        mutate(MSSubClass = as.character(MSSubClass)) %>%
        define_dummies()

    # Impute missing values in numeric predictors.
    data <- impute_data(data, verbose)

    data <- apply_transformations(data)

    # Encode categorical variables.
    data <- encode_predictors(data, 'mean',
                              drop_categorical = drop_categorical)

    return(data)
}

check_vector_lengths <- function(predicted, actual) {
    num_rows <- length(predicted)
    if (num_rows != length(actual)) {
        message <- paste0(
            'In check_vector_lengths, there is a mismatch in the vector ',
            'lengths.\npredicted:  ', num_rows, '\nactual:  ', length(actual),
            '\n'
        )
        stop(message)
    }
    return(TRUE)
}

# The functions get_rmse and get_rmsle evaluate the root-mean-square error and
# the room-mean-square logarithmic error.  The argument apply_exp should be
# TRUE if the target was transformed by taking the log.
get_rmse <- function(predicted, actual, apply_exp = TRUE) {
    check_vector_lengths(predicted, actual)
    if (apply_exp) {
        predicted <- exp(predicted)
        actual <- exp(actual)
    }
    num_rows <- length(predicted)
    mse <- sum((predicted - actual)^2) / num_rows
    return(sqrt(mse))
}

get_rmsle <- function(predicted, actual, apply_exp = TRUE) {
    check_vector_lengths(predicted, actual)
    if (apply_exp) {
        predicted <- exp(predicted)
        actual <- exp(actual)
    }
    num_rows <- length(predicted)
    msle <- sum((log(predicted + 1) - log(actual + 1))^2) / num_rows
    return(sqrt(msle))
}

report_lm_metrics <- function(glmnet_inputs) {
    predicted <- as.numeric(predict(
        lasso_models,
        s = best_lambda,
        newx = glmnet_inputs$x,
        type = 'response'
    ))
    rmse <- get_rmse(predicted = predicted, actual = glmnet_inputs$y)
    rmsle <- get_rmsle(predicted = predicted, actual = glmnet_inputs$y)
    cat('root-mean-square error:  ', round(rmse, digits = 2),
        '\nroot-mean-square logarithmic error:  ', round(rmsle, digits = 3),
        '\n\n', sep = '')
}

# Boolean that determins whether a prediction for the test data is generated
# when the markdown file is rendered.
predict_linear <- FALSE
if (predict_linear) {
    test_data <- preprocess_lm(test_data)
    glmnet_inputs <- get_glmnet_inputs(test_data, best_lm_predictors)
    linear_prediction <- exp(as.numeric(predict(
        lasso_models,
        s = best_lambda,
        newx = glmnet_inputs$x,
        type = 'response'
    )))
    to_save <- data.frame(Id = test_data$Id, SalePrice = linear_prediction)
    write.csv(to_save, 'data/linear_prediction.csv',
              row.names = FALSE, quote = FALSE)
}

# Curly braces used to force output into a single block in the rendered
# markdown.
{
    cat('Error metrics for the data used to train the model ',
        '(3 outliers excluded):\n\n',
        sep = '')
    report_lm_metrics(
        get_glmnet_inputs(train_data, best_lm_predictors, outliers = outliers)
    )
    cat('Error metrics for the full "training data set" ',
        '(no outliers excluded):\n\n',
        sep = '')
    report_lm_metrics(
        get_glmnet_inputs(train_data, best_lm_predictors)
    )

    cat('\nFor the test data set, the root-mean-square logarithmic ',
        'error was 0.12963.',
        sep = '')
}

# Save predicted values and residuals to be used in comparing the multilinear
# model and the random-forest model.
get_lm_result <- function() {
    glmnet_inputs <- get_glmnet_inputs(train_data, best_lm_predictors)
    predicted <- as.numeric(predict(
        lasso_models,
        s = best_lambda,
        newx = glmnet_inputs$x,
        type = 'response'
    ))
    residuals <- train_data$SalePrice - predicted
    result <- list(
        predicted = predicted,
        residuals = residuals
    )
    return(result)
}
lm_result <- get_lm_result()
```

</div>

## Possible improvements {.tabset}

### Summary {.unnumbered}

Discuss work that could be done to improve the multilinear model.

### Discussion {.unnumbered}

Feature engineering for Condition1 and Condition2:

  - These two variables both give information about "proximity to various
    conditions," such as parks or feeder streets.  The variable Condition1 was
    selected as important for the model, while condition2 was not.  From the
    visualization of Condition1, it is clear that some of these conditions are
    more important than others.  It is possible that for some houses with two
    conditions, the information available in Condition1 may be less important
    than the information in Condition2.  The model might be improved by defining
    a predictor that assigns a single condition to each house, choosing the most
    important when two conditions are present.

Research to understand important variables.  Examples:

  - Condition1 has four categories associated with nearness to railroads.
    These distinguish between north-south railroads and east-west railroads,
    as well as between houses that are adjacent to a railroad or within 200 ft
    of a railroad.  Of these four categories, the only one that clearly shows a
    price penalty is 'adjacent to east-west-railroad.'  Is there an east-west
    railroad that is particularly active?  Or is this just a random effect
    associated with the fact that the number of houses in these categories is
    relatively small.
  - Neighborhood has a relative importance of about 30%.  What are the
    underlying reasons that some neighborhoods are preferred over others?

Better handling of outliers:

  - Even with three outliers excluded, the training data has a significant
    number of observations with large residuals, and these may be distorting the
    model predictions.  Using a loss function such as the Huber loss that is
    robust to large residuals might give a better model by minimizing such
    distortions.

Better handling of dummy variables defined during feature engineering:

  - Stepwise regression tests the improvement associated with adding or
    subtracting individual variables.  This process ignores they way in which
    combinations of two variables improve the model, e.g., NoGarage and
    GarageArea.  Dummy variables such as NoGarage were defined specifically to
    improve the fit for related numerical variables, but the stepwise regression
    used for feature selection was "unaware" of this aspect of the dummy
    variables.  This might be corrected by exploring different starting models
    for the stepwise regression.

## Qualitative model {.tabset}

### Summary {.unnumbered}

Create a simple qualitative summary of the model.

### Discussion {.unnumbered}

The multilinear model includes 25 predictors, some of which are much less
important than others, and so the "take-home messages" from the model may not
be apparent.

  - Example:  With standardized inputs, the coefficient for WoodDeckSf is
    smaller than the coefficient for GrLivArea by a factor of about 5000.

Extract the most important features from the model, and group these features
into three levels by importance.

  - In reporting the results, suppress the technical distinction between the
    original variables and the transformed or encoded variables used in the
    multilinear model.
  - Note the training process includes a random element, and so the relative
    importance attached to each variable changes somewhat with each iteration
    of the training.  As a result, the values shown in this subsection are
    somewhat different from the hard-coded values shown in Sec 1.1.

### Primary variables {.tab-display .unnumbered}

<div class="code-block">

```{r, primary, cache=TRUE}
coef_relative_importance <- standardized_coeffs / standardized_coeffs[1]
extract_by_importance <- function(lower, upper) {
    bool_index <- (
        (coef_relative_importance <= upper) & (coef_relative_importance > lower)
    )
    extracted <- coef_relative_importance[bool_index]
    names(extracted) <- names(extracted) %>%
        str_replace('Num', '')
    return(extracted)
}
first_importance <- extract_by_importance(0.35, 1.0)
second_importance <- extract_by_importance(0.2, 0.35)
third_importance <- extract_by_importance(0.1, 0.2)


report_summary(first_importance)
```

</div>

### Secondary variables {.tab-display .unnumbered}

<div class="code-block">

```{r, secondary, cache=TRUE}
report_summary(second_importance)
```

</div>

### Minor variables {.tab-display .unnumbered}

<div class="code-block">

```{r, minor, cache=TRUE}
report_summary(third_importance)
```

</div>
